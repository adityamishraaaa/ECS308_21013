# -*- coding: utf-8 -*-
"""Presentation_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G5g9MA_HaGR9sDStijWhIXA-XG7YD0Aj

## **Diabetes Prediction over Telephonic Health Survey using Machine Learning**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df_train_features = pd.read_csv("/content/drive/MyDrive/ML Project Data/training_data.csv")
df_train_features.head()

df_train_target=pd.read_csv("/content/drive/MyDrive/ML Project Data/training_data_targets.csv", names=['output'])
df_train_target.head()

df = pd.concat([df_train_features,df_train_target], axis = 1)
df

df['output']=df['output'].astype(int)

class_counts = df['output'].value_counts()
print(class_counts)

"""# **Data Visualization**"""

import seaborn as sns
import matplotlib.pyplot as plt

# Class distribution values
class_distribution_values = {'Class 0': 192333, 'Class 1': 4168,'Class 2': 31811}

# Plot a horizontal bar chart using seaborn
plt.figure(figsize=(5, 5))
sns.barplot(x=list(class_distribution_values.keys()), y=list(class_distribution_values.values()), palette='viridis', width = 0.6)

# Add labels and title
plt.xlabel('Class')
plt.ylabel('Number of Instances')
plt.title('Class Distribution')

# Add values on the bars
for index, value in enumerate(class_distribution_values.values()):
    plt.text(index, value, str(value), ha='center', va='bottom')


plt.show()

"""## **Dataset Handling - Imputation, Class Imbalance and Feature Importance**

# **Correlation Heatmap**
"""

import seaborn as sns

# Compute the correlation matrix
correlation_matrix = df.corr()

plt.figure(figsize=(20,20))
# Create a correlation heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm',  fmt=".2f", xticklabels=correlation_matrix.columns, yticklabels=correlation_matrix.columns)

# Show the plot
#plt.colorbar()
plt.show()

from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif
from sklearn.model_selection import train_test_split



# Split the dataset into input (X) and output (y) variables
X = df.drop('output', axis=1)
y = df['output']

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=df['output'],test_size=0.1, random_state=1)

# Perform feature selection using mutual information
fs = SelectKBest(score_func=chi2, k='all')
fs.fit(X_train, y_train)

# Get the selected features and their scores
feature_scores = pd.DataFrame({'Feature': X.columns, 'Score': fs.scores_})
feature_scores = feature_scores.sort_values(by='Score', ascending=False)


# Print the feature names and scores
for idx, row in feature_scores.iterrows():
    print(f"{row['Feature']} {row['Score']}")



# Plot the feature scores
plt.figure(figsize=(14, 8))
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('')
plt.barh(feature_scores['Feature'],feature_scores['Score'])
#pyplot.xticks(rotation='vertical')

"""# **SPLIT**"""

from sklearn.model_selection import train_test_split
df_train, df_val = train_test_split(df, test_size=0.2, stratify=df['output'],random_state=42)

"""# **Oversampling**"""

import pandas as pd

minority_class1 = df_train[df_train['output'] == 1]
minority_class2 = df_train[df_train['output'] == 2]

oversampling_ratio1 = 20
oversampling_ratio2 = 2

oversampled_minority_class1 = pd.concat([minority_class1] * oversampling_ratio1, ignore_index=True)
oversampled_minority_class2 = pd.concat([minority_class2] * oversampling_ratio2, ignore_index=True)

df_train_oversampled = pd.concat([df_train, oversampled_minority_class1,oversampled_minority_class2], ignore_index=True)

df_train_oversampled['output'].value_counts()

import seaborn as sns
import matplotlib.pyplot as plt

# Class distribution values
class_distribution_values = {'Class 0': 153866, 'Class 1': 70014,'Class 2': 76347}

# Plot a horizontal bar chart using seaborn
plt.figure(figsize=(5, 5))
sns.barplot(x=list(class_distribution_values.keys()), y=list(class_distribution_values.values()), palette='viridis', width=0.4)

# Add labels and title
plt.xlabel('Class')
plt.ylabel('Number of Instances')
plt.title('Class Distribution after RatioOversampler')

# Add values on the bars
for index, value in enumerate(class_distribution_values.values()):
    plt.text(index, value, str(value), ha='center', va='bottom')

plt.show()

"""**Other techniques: SMOTE/Adasyn**

# **SMOTE**
"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

X_train = pd.DataFrame({
    'GenHlth': df_train['GenHlth'],
    'HighBP': df_train['HighBP'],
    'BMI': df_train['BMI'],
    'Age': df_train['Age'],
    'HeartDiseaseorAttack': df_train['HeartDiseaseorAttack'],
    'Income': df_train['Income'],
    'PhysActivity': df_train['PhysActivity'],
    'Stroke': df_train['Stroke']

})
y_train = df_train['output']

# Initialize the SMOTE object
smote = SMOTE(sampling_strategy='auto', random_state=42)

# Fit and resample the training data
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

y_train_smote.value_counts()

"""# **Random Oversampler**"""

import pandas as pd
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split

# Assuming 'df_train' is your training DataFrame with 'output' as the target column
X_train = df_train.drop('output', axis=1)
y_train = df_train['output']

# Initialize the RandomOverSampler
ros = RandomOverSampler(random_state=42)

# Fit and resample the training data
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)

# Convert the resampled data back to a DataFrame if needed
df_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)
df_train_resampled['output'] = y_train_resampled

df_train_resampled['output'].value_counts()

"""# **ML Models**

# **1. Logistic Regression**
"""

X_train = pd.DataFrame({
    'GenHlth': df_train_oversampled['GenHlth'],
    'HighBP': df_train_oversampled['HighBP'],
    'BMI': df_train_oversampled['BMI'],
    'Age': df_train_oversampled['Age'],
    'HeartDiseaseorAttack': df_train_oversampled['HeartDiseaseorAttack'],
    'Income': df_train_oversampled['Income'],
    'PhysActivity': df_train_oversampled['PhysActivity'],
    'Stroke': df_train_oversampled['Stroke']
})
y_train = df_train_oversampled['output']

X_val = pd.DataFrame({
    'GenHlth': df_val['GenHlth'],
    'HighBP': df_val['HighBP'],
    'BMI': df_val['BMI'],
    'Age': df_val['Age'],
    'HeartDiseaseorAttack': df_val['HeartDiseaseorAttack'],
    'Income': df_val['Income'],
    'PhysActivity': df_val['PhysActivity'],
    'Stroke': df_val['Stroke']
})
y_val = df_val['output']

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.fit_transform(X_val)

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(class_weight='balanced',penalty='l2',fit_intercept=True,max_iter=1000,multi_class='multinomial',solver='lbfgs',random_state=0)
log.fit(X_train_scaled, y_train)

pred_log = log.predict(X_val_scaled)

from sklearn.metrics import classification_report

lgb_report = classification_report(y_val, pred_log, target_names=['Class 0', 'Class 1', 'Class 2'])
print("Classification Report for Logistic Regression:\n", lgb_report)

from sklearn.metrics import precision_score, recall_score, f1_score

log_macro_precision = precision_score(y_val, pred_log, average='macro')
log_macro_recall = recall_score(y_val, pred_log, average='macro')
log_macro_f1 = f1_score(y_val, pred_log,average='macro')
print(f"Macro-Averaged Precision: {log_macro_precision:.2f}")
print(f"Macro-Averaged Recall: {log_macro_recall:.2f}")
print(f"Macro-Averaged F1 Score: {log_macro_f1:.2f}")

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_val, pred_log)

print("Confusion Matrix:")
print(conf_matrix)

"""# **2. Random Forest**"""

X_train = pd.DataFrame({
    'GenHlth': df_train_oversampled['GenHlth'],
    'HighBP': df_train_oversampled['HighBP'],
    'BMI': df_train_oversampled['BMI'],
    'Age': df_train_oversampled['Age'],
    'HeartDiseaseorAttack': df_train_oversampled['HeartDiseaseorAttack'],
    'Income': df_train_oversampled['Income'],
    'PhysActivity': df_train_oversampled['PhysActivity'],
    'Stroke': df_train_oversampled['Stroke']
})
y_train = df_train_oversampled['output']

X_val = pd.DataFrame({
    'GenHlth': df_val['GenHlth'],
    'HighBP': df_val['HighBP'],
    'BMI': df_val['BMI'],
    'Age': df_val['Age'],
    'HeartDiseaseorAttack': df_val['HeartDiseaseorAttack'],
    'Income': df_val['Income'],
    'PhysActivity': df_val['PhysActivity'],
    'Stroke': df_val['Stroke']
})
y_val = df_val['output']

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Create the Random Forest Classifier
rf_classifier = RandomForestClassifier()

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 300, 500, 800, 1000],
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=2, scoring='f1_macro')

# Fit the model to the data
grid_search.fit(X_train, y_train)

# Get the best parameters and the best model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Print the best parameters
print("Best Parameters:", best_params)

# Train the model with the best parameters
best_model.fit(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=1000,criterion='entropy',max_depth=None,min_samples_split=5)
model.fit(X_train,y_train)

val_predict = model.predict(X_val)

from sklearn.metrics import precision_score, recall_score, f1_score

from sklearn.metrics import classification_report

lgb_report = classification_report(y_val, val_predict, target_names=['Class 0', 'Class 1', 'Class 2'])
print("Classification Report for LightGBM:\n", lgb_report)


# Calculate precision, recall, and F1 score with 'macro' averaging
precision_macro = precision_score(y_val, val_predict, average='macro')
recall_macro = recall_score(y_val, val_predict, average='macro')
f1_macro = f1_score(y_val, val_predict, average='macro')
print("Macro Precision:", precision_macro)
print("Macro Recall:", recall_macro)
print("Macro F1 Score:", f1_macro)

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_val, val_predict)

print("Confusion Matrix:")
print(conf_matrix)

"""# **2. K-Nearest Neighbour**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Initialize the K-NN classifier with your desired number of neighbors (k)
k = 10  # You can adjust this value as needed
knn_classifier = KNeighborsClassifier(n_neighbors=k, weights='distance')

# Fit the classifier on the training data
knn_classifier.fit(X_train, y_train)

# Predict the class labels for the val data
y_pred = knn_classifier.predict(X_val)

from sklearn.metrics import precision_score, recall_score, f1_score

pr_knn = precision_score(y_val, y_pred, average='macro')
rcl_knn = recall_score(y_val, y_pred, average='macro')
f1_knn = f1_score(y_val, y_pred, average='macro')

print(f"Validation Data Metrics:")
print(f"Macro-Averaged Precision: {pr_knn:.4f}")
print(f"Macro-Averaged Recall: {rcl_knn:.4f}")
print(f"Macro-Averaged F1 Score: {f1_knn:.4f}")

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_val, y_pred)

print("Confusion Matrix:")
print(conf_matrix)

"""## **3. MLP**

### Refer to other file for results, as it was taking too long to exceute due to 1000 iterations.

# **4. Naive Bayes**
"""

from sklearn.naive_bayes import GaussianNB
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)
y_pred_nb = nb_classifier.predict(X_val)

from sklearn.metrics import precision_score, recall_score, f1_score

nb_macro_precision = precision_score(y_val, y_pred_nb, average='macro')
nb_macro_recall = recall_score(y_val, y_pred_nb, average='macro')
nb_macro_f1 = f1_score(y_val, y_pred_nb, average='macro')
print(f"Macro-Averaged Precision for NBC: {nb_macro_precision:.4f}")
print(f"Macro-Averaged Recall for NBC: {nb_macro_recall:.4f}")
print(f"Macro-Averaged F1 Score for NBC: {nb_macro_f1:.4f}")

"""# **5. Adaboost**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

# Initialize the Random Forest classifier as the base estimator for AdaBoost
base_classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)
#base_classifier = GaussianNB()
#base_classifier = LogisticRegression(class_weight='balanced',penalty='l2',fit_intercept=True,random_state=0)



# Initialize the AdaBoost classifier
adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=500, random_state=42)

# Fit the classifier on the training data
adaboost_classifier.fit(X_train, y_train)

# Predict the class labels for the test data
y_pred_adaboost = adaboost_classifier.predict(X_val)

report = classification_report(y_val, y_pred_adaboost)
print("Classification Report:\n", report)

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_val, y_pred_adaboost)

print("Confusion Matrix:")
print(conf_matrix)

"""# **6. LightGBM**"""

X_train = pd.DataFrame({
    'GenHlth': df_train_oversampled['GenHlth'],
    'HighBP': df_train_oversampled['HighBP'],
    'BMI': df_train_oversampled['BMI'],
    'Age': df_train_oversampled['Age'],
    'HeartDiseaseorAttack': df_train_oversampled['HeartDiseaseorAttack'],
    'Income': df_train_oversampled['Income'],
    'PhysActivity': df_train_oversampled['PhysActivity'],
    'Stroke': df_train_oversampled['Stroke']
})
y_train = df_train_oversampled['output']

X_val = pd.DataFrame({
    'GenHlth': df_val['GenHlth'],
    'HighBP': df_val['HighBP'],
    'BMI': df_val['BMI'],
    'Age': df_val['Age'],
    'HeartDiseaseorAttack': df_val['HeartDiseaseorAttack'],
    'Income': df_val['Income'],
    'PhysActivity': df_val['PhysActivity'],
    'Stroke': df_val['Stroke']
})
y_val = df_val['output']

import lightgbm as lgb
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score

# Initialize the LightGBM classifier
lgb_classifier = LGBMClassifier(n_estimators=2000, num_leaves=31, learning_rate=0.1,random_state=42)

# Fit the classifier on the training data
lgb_classifier.fit(X_train_smote, y_train_smote)

y_pred_lgb = lgb_classifier.predict(X_val)
#y_pred_train_lgb = lgb_classifier.predict(X_train)

from sklearn.metrics import classification_report

lgb_report = classification_report(y_val, y_pred_lgb, target_names=['Class 0', 'Class 1', 'Class 2'])
print("Classification Report for LightGBM:\n", lgb_report)

from sklearn.metrics import precision_score, recall_score, f1_score

lgb_macro_precision = precision_score(y_val, y_pred_lgb, average='macro')
lgb_macro_recall = recall_score(y_val, y_pred_lgb, average='macro')
lgb_macro_f1 = f1_score(y_val, y_pred_lgb, average='macro')
print(f"Macro-Averaged Precision for LightGBM: {lgb_macro_precision:.2f}")
print(f"Macro-Averaged Recall for LightGBM: {lgb_macro_recall:.2f}")
print(f"Macro-Averaged F1 Score for LightGBM: {lgb_macro_f1:.2f}")

from sklearn.metrics import classification_report

lgb_report = classification_report(y_train, y_pred_train_lgb, target_names=['Class 0', 'Class 1', 'Class 2'])
print("Classification Report for LightGBM:\n", lgb_report)

from sklearn.metrics import precision_score, recall_score, f1_score

lgb_macro_precision_train = precision_score(y_train, y_pred_train_lgb, average='macro')
lgb_macro_recall_train = recall_score(y_train, y_pred_train_lgb, average='macro')
lgb_macro_f1_train = f1_score(y_train, y_pred_train_lgb, average='macro')
print(f"Macro-Averaged Precision for LightGBM: {lgb_macro_precision_train:.2f}")
print(f"Macro-Averaged Recall for LightGBM: {lgb_macro_recall_train:.2f}")
print(f"Macro-Averaged F1 Score for LightGBM: {lgb_macro_f1_train:.2f}")

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_val, y_pred_lgb)

print("Confusion Matrix:")
print(conf_matrix)

"""**I have also used models like ANN, Extreme Gradient Boosting (XGB) and Decision Tree, but the results were not up to the mark. Their performance are mentioned in other file.**

# **With Random Oversampler**
"""

X_train = pd.DataFrame({
    'GenHlth': df_train_resampled['GenHlth'],
    'HighBP': df_train_resampled['HighBP'],
    'BMI': df_train_resampled['BMI'],
    'Age': df_train_resampled['Age'],
    'HeartDiseaseorAttack': df_train_resampled['HeartDiseaseorAttack'],
    'Income': df_train_resampled['Income'],
    'PhysActivity': df_train_resampled['PhysActivity'],
    'Stroke': df_train_resampled['Stroke']
})
y_train = df_train_resampled['output']

X_val = pd.DataFrame({
    'GenHlth': df_val['GenHlth'],
    'HighBP': df_val['HighBP'],
    'BMI': df_val['BMI'],
    'Age': df_val['Age'],
    'HeartDiseaseorAttack': df_val['HeartDiseaseorAttack'],
    'Income': df_val['Income'],
    'PhysActivity': df_val['PhysActivity'],
    'Stroke': df_val['Stroke']
})
y_val = df_val['output']

import lightgbm as lgb
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score

# Initialize the LightGBM classifier
lgb_classifier2 = LGBMClassifier(n_estimators=1000, num_leaves=50, learning_rate=0.1,random_state=42) #,num_leaves=31,learning_rate=0.1

# Fit the classifier on the training data
lgb_classifier2.fit(X_train, y_train)

y_pred_lgb2 = lgb_classifier2.predict(X_val)

from sklearn.metrics import classification_report

lgb_report = classification_report(y_val, y_pred_lgb2, target_names=['Class 0', 'Class 1', 'Class 2'])
print("Classification Report for LightGBM:\n", lgb_report)

from sklearn.metrics import precision_score, recall_score, f1_score

lgb_macro_precision = precision_score(y_val, y_pred_lgb2, average='macro')
lgb_macro_recall = recall_score(y_val, y_pred_lgb2, average='macro')
lgb_macro_f1 = f1_score(y_val, y_pred_lgb2, average='macro')
print(f"Macro-Averaged Precision for LightGBM: {lgb_macro_precision:.2f}")
print(f"Macro-Averaged Recall for LightGBM: {lgb_macro_recall:.2f}")
print(f"Macro-Averaged F1 Score for LightGBM: {lgb_macro_f1:.2f}")

"""# **Experiment on LightGBM**"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Assuming 'df' is your main DataFrame with the data

# Identify the minority class
minority_class = df['output'].value_counts().idxmin()

# Select k instances for training from each class
k = int(0.9 * df[df['output'] == minority_class].shape[0])
df_train = pd.DataFrame()

for class_label in df['output'].unique():
    class_instances = df[df['output'] == class_label]
    class_train, _ = train_test_split(class_instances, test_size=(1 - k / class_instances.shape[0]), random_state=42)
    df_train = pd.concat([df_train, class_train])

# Select t instances for validation from each class
t = int(0.1 * df[df['output'] == minority_class].shape[0])
df_val = pd.DataFrame()

for class_label in df['output'].unique():
    class_instances = df[df['output'] == class_label]
    class_val, _ = train_test_split(class_instances, test_size=(1 - t / class_instances.shape[0]), random_state=42)
    df_val = pd.concat([df_val, class_val])

# Reset index for the new DataFrames
df_train.reset_index(drop=True, inplace=True)
df_val.reset_index(drop=True, inplace=True)

df_train['output'].value_counts()

df_val['output'].value_counts()

X_train = df_train.drop('output', axis=1)
y_train = df_train['output']
X_val = df_val.drop('output',axis=1)
y_val = df_val['output']

import lightgbm as lgb
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score

# Initialize the LightGBM classifier
lgb_classifier3 = LGBMClassifier(n_estimators=1000, num_leaves=50, learning_rate=0.1,random_state=42)

# Fit the classifier on the training data
lgb_classifier3.fit(X_train, y_train)

y_pred_lgb3 = lgb_classifier3.predict(X_val)

from sklearn.metrics import classification_report

lgb_report = classification_report(y_val, y_pred_lgb3, target_names=['Class 0', 'Class 1', 'Class 2'])
print("Classification Report for LightGBM:\n", lgb_report)

from sklearn.metrics import precision_score, recall_score, f1_score

lgb_macro_precision = precision_score(y_val, y_pred_lgb3, average='macro')
lgb_macro_recall = recall_score(y_val, y_pred_lgb3, average='macro')
lgb_macro_f1 = f1_score(y_val, y_pred_lgb3, average='macro')
print(f"Macro-Averaged Precision for LightGBM: {lgb_macro_precision:.2f}")
print(f"Macro-Averaged Recall for LightGBM: {lgb_macro_recall:.2f}")
print(f"Macro-Averaged F1 Score for LightGBM: {lgb_macro_f1:.2f}")

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_val, y_pred_lgb3)

print("Confusion Matrix:")
print(conf_matrix)

"""# **Bagging**"""

import lightgbm as lgb
from lightgbm import LGBMClassifier

base_classifier = LGBMClassifier(n_estimators=1000, num_leaves=50, learning_rate=0.1,eval_set=[(X_val, y_val)],random_state=42)

from sklearn.ensemble import BaggingClassifier
# bagging_params = {
#     'base_estimator': base_classifier,
# }
classifier = BaggingClassifier(base_classifier,n_estimators=10,max_samples=10,verbose=1,n_jobs=-1)
classifier.fit(X_train,y_train)

y_pred_bg = classifier.predict(X_val)

from sklearn.metrics import classification_report

lgb_report = classification_report(y_val, y_pred_bg, target_names=['Class 0', 'Class 1', 'Class 2'])
print("Classification Report for LightGBM:\n", lgb_report)

from sklearn.metrics import precision_score, recall_score, f1_score

bg_macro_precision = precision_score(y_val, y_pred_bg, average='macro')
bg_macro_recall = recall_score(y_val, y_pred_bg, average='macro')
bg_macro_f1 = f1_score(y_val, y_pred_bg, average='macro')
print(f"Macro-Averaged Precision for LightGBM: {bg_macro_precision:.4f}")
print(f"Macro-Averaged Recall for LightGBM: {bg_macro_recall:.4f}")
print(f"Macro-Averaged F1 Score for LightGBM: {bg_macro_f1:.4f}")

"""# **Hyperparameterting Tuning - LightGBM**"""

import lightgbm as lgb
from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV

# Initialize the LightGBM classifier
lgb_classifier = LGBMClassifier(eval_set=[(X_val, y_val)],random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [100,200,500],
    'num_leaves': [10, 20,31],
    'learning_rate': [0.05, 0.1, 0.01]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=lgb_classifier, param_grid=param_grid, scoring='f1_macro',n_jobs=-1, cv=2)

# Fit the classifier using grid search
grid_search.fit(X_train, y_train, eval_set=[(X_val, y_val)])

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Get the best estimator
best_estimator = grid_search.best_estimator_

# Use the best estimator for predictions
y_pred_lgb = best_estimator.predict(X_val)

print("Best Parameters:", best_params)

"""# **K-Means**"""

# K-means Clustering Technique:

from sklearn.cluster import KMeans
data = df[['HighBP', 'HighChol','CholCheck','BMI']]
num = 3

kmeans = KMeans(n_clusters=num)
kmeans.fit(data)

df['Cluster'] = kmeans.labels_

from sklearn.metrics import normalized_mutual_info_score

# Replace these with your actual cluster labels
true_labels = df['output']
predicted_labels = df['Cluster']

# Calculate NMI
nmi = normalized_mutual_info_score(true_labels, predicted_labels)

print(f"Normalized Mutual Information (NMI): {nmi:.4f}")

"""# **ANN**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)

# Define the model
model = Sequential()

# Add three hidden layers with relu activation
model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))

# Add the output layer with softmax activation for multi-class classification
# The number of units should be equal to the number of classes
model.add(Dense(3, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['f1_macro'])

# Train the model
#model.fit(X_train_scaled, y_train, epochs=3, batch_size=32, validation_split=0.2)
model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_split=0.2)

y_predict_probs = model.predict(X_val_scaled)
y_predict_val = np.argmax(y_predict_probs, axis=1)

from sklearn.metrics import classification_report

ann_report = classification_report(y_val, y_predict_val, target_names=['Class 0', 'Class 1', 'Class 2'])
print("Classification Report for ANN:\n", ann_report)

from sklearn.metrics import precision_score, recall_score, f1_score

ann_macro_precision = precision_score(y_val, y_predict_val, average='macro')
ann_macro_recall = recall_score(y_val, y_predict_val, average='macro')
ann_macro_f1 = f1_score(y_val, y_predict_val, average='macro')
print(f"Macro-Averaged Precision for LightGBM: {ann_macro_precision:.2f}")
print(f"Macro-Averaged Recall for LightGBM: {ann_macro_recall:.2f}")
print(f"Macro-Averaged F1 Score for LightGBM: {ann_macro_f1:.2f}")

from sklearn.metrics import confusion_matrix

confusion = confusion_matrix(y_test, y_predict_val)

print("Confusion Matrix for ANN:")
print(confusion)